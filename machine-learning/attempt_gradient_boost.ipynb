{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c859408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer, f1_score, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37fcf068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHARGEMENT DES DONNÉES ===\n",
      "================================================================================\n",
      "\n",
      "5 Premières lignes:\n",
      "      PatientId  AppointmentID Gender          ScheduledDay  \\\n",
      "0  2.987250e+13        5642903      F  2016-04-29T18:38:08Z   \n",
      "1  5.589978e+14        5642503      M  2016-04-29T16:08:27Z   \n",
      "2  4.262962e+12        5642549      F  2016-04-29T16:19:04Z   \n",
      "3  8.679512e+11        5642828      F  2016-04-29T17:29:31Z   \n",
      "4  8.841186e+12        5642494      F  2016-04-29T16:07:23Z   \n",
      "\n",
      "         AppointmentDay  Age      Neighbourhood  Scholarship  Hypertension  \\\n",
      "0  2016-04-29T00:00:00Z   62    JARDIM DA PENHA            0             1   \n",
      "1  2016-04-29T00:00:00Z   56    JARDIM DA PENHA            0             0   \n",
      "2  2016-04-29T00:00:00Z   62      MATA DA PRAIA            0             0   \n",
      "3  2016-04-29T00:00:00Z    8  PONTAL DE CAMBURI            0             0   \n",
      "4  2016-04-29T00:00:00Z   56    JARDIM DA PENHA            0             1   \n",
      "\n",
      "   Diabetes  Alcoholism  Handicap  SMS_received No-show  \n",
      "0         0           0         0             0      No  \n",
      "1         0           0         0             0      No  \n",
      "2         0           0         0             0      No  \n",
      "3         0           0         0             0      No  \n",
      "4         1           0         0             0      No  \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. CHARGEMENT DES DONNÉES\n",
    "print(\"=== CHARGEMENT DES DONNÉES ===\")\n",
    "\n",
    "try:\n",
    "    # https://www.kaggle.com/datasets/joniarroba/noshowappointments/data\n",
    "    path = \"no-show.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    # rename car y a des titres mal orthographiés\n",
    "    df = df.rename(columns={'Handcap': 'Handicap', 'Hipertension': 'Hypertension'})\n",
    "\n",
    "\n",
    "except:\n",
    "    print(\"Erreur lors du chargement des données\")\n",
    "    \n",
    "print( \"=\" * 80)\n",
    "print(\"\\n5 Premières lignes:\")\n",
    "print(df.head())\n",
    "print( \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff131c94",
   "metadata": {},
   "source": [
    "## STRATEGIE : 2° EXPLORATION RAPIDE\n",
    "\n",
    "Mettre les metrics et les moyennes générales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c03019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLORATION RAPIDE DES DONNÉES ===\n",
      "Forme du dataset: (110527, 14)\n",
      "\n",
      "\n",
      "Colonnes: ['PatientId', 'AppointmentID', 'Gender', 'ScheduledDay', 'AppointmentDay', 'Age', 'Neighbourhood', 'Scholarship', 'Hypertension', 'Diabetes', 'Alcoholism', 'Handicap', 'SMS_received', 'No-show']\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "=> Types de données et valeurs manquantes:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110527 entries, 0 to 110526\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   PatientId       110527 non-null  float64\n",
      " 1   AppointmentID   110527 non-null  int64  \n",
      " 2   Gender          110527 non-null  object \n",
      " 3   ScheduledDay    110527 non-null  object \n",
      " 4   AppointmentDay  110527 non-null  object \n",
      " 5   Age             110527 non-null  int64  \n",
      " 6   Neighbourhood   110527 non-null  object \n",
      " 7   Scholarship     110527 non-null  int64  \n",
      " 8   Hypertension    110527 non-null  int64  \n",
      " 9   Diabetes        110527 non-null  int64  \n",
      " 10  Alcoholism      110527 non-null  int64  \n",
      " 11  Handicap        110527 non-null  int64  \n",
      " 12  SMS_received    110527 non-null  int64  \n",
      " 13  No-show         110527 non-null  object \n",
      "dtypes: float64(1), int64(8), object(5)\n",
      "memory usage: 11.8+ MB\n",
      "None\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=> Valeurs manquantes:\n",
      "\n",
      "PatientId         0\n",
      "AppointmentID     0\n",
      "Gender            0\n",
      "ScheduledDay      0\n",
      "AppointmentDay    0\n",
      "Age               0\n",
      "Neighbourhood     0\n",
      "Scholarship       0\n",
      "Hypertension      0\n",
      "Diabetes          0\n",
      "Alcoholism        0\n",
      "Handicap          0\n",
      "SMS_received      0\n",
      "No-show           0\n",
      "dtype: int64\n",
      "\n",
      "=> PAS DE CELLULES VIDES !!\n",
      "================================================================================\n",
      "\n",
      "=== METRICS GENERALES ===\n",
      "\n",
      "=> Taux de No Show TOTAL : 22319/110527 = 20.19%\n",
      "=> Nombre de Femmes : 71840/110527 = 65.00%\n",
      "=> Population Ayant un certain niveau d'études Scholarship : 10861/110527 = 9.83%\n",
      "=> population Ayant de l'hypertension : 21801/110527 = 19.72%\n",
      "=> population Ayant reçu un SMS : 35482/110527 = 32.10%\n",
      "=> population Ayant un handicap : 2241/110527 = 2.03%\n",
      "=> population Ayant un alcoolisme : 3360/110527 = 3.04%\n",
      "=> population Ayant un diabète : 7943/110527 = 7.19%\n"
     ]
    }
   ],
   "source": [
    "# 2. EXPLORATION RAPIDE\n",
    "print(\"\\n=== EXPLORATION RAPIDE DES DONNÉES ===\")\n",
    "# Afficher la forme du dataset et les colonnes\n",
    "print(f\"Forme du dataset: {df.shape}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Colonnes: {list(df.columns)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Explorer les types de données et les valeurs manquantes\n",
    "print(\"\\n\")\n",
    "print(\"\\n=> Types de données et valeurs manquantes:\\n\")\n",
    "print(df.info())\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Afficher les statistiques descriptives\n",
    "print(\"\\n\")\n",
    "print(\"=> Valeurs manquantes:\\n\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n=> PAS DE CELLULES VIDES !!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n=== METRICS GENERALES ===\")\n",
    "# Calcul du taux de No-show\n",
    "total_rows = len(df)\n",
    "no_show_count = (df['No-show'] == 'Yes').sum()\n",
    "no_show_rate = (no_show_count / total_rows) * 100\n",
    "print(f\"\\n=> Taux de No Show TOTAL : {no_show_count}/{total_rows} = {no_show_rate:.2f}%\")\n",
    "\n",
    "# Nombre de femmes\n",
    "femmes_count = (df['Gender'] == 'F').sum()\n",
    "femmes_rate = (femmes_count / total_rows) * 100\n",
    "print(f\"=> Nombre de Femmes : {femmes_count}/{total_rows} = {femmes_rate:.2f}%\")\n",
    "\n",
    "# Nombre avec Scholarship\n",
    "scholarship_count = (df['Scholarship'] == 1).sum()\n",
    "scholarship_rate = (scholarship_count / total_rows) * 100\n",
    "print(f\"=> Population Ayant un certain niveau d'études Scholarship : {scholarship_count}/{total_rows} = {scholarship_rate:.2f}%\")\n",
    "\n",
    "# Nombre avec Hypertension\n",
    "hypertension_count = (df['Hypertension'] == 1).sum()\n",
    "hypertension_rate = (hypertension_count / total_rows) * 100\n",
    "print(f\"=> population Ayant de l'hypertension : {hypertension_count}/{total_rows} = {hypertension_rate:.2f}%\")\n",
    "\n",
    "# Nombre avec SMS reçu\n",
    "sms_count = (df['SMS_received'] == 1).sum()\n",
    "sms_rate = (sms_count / total_rows) * 100\n",
    "print(f\"=> population Ayant reçu un SMS : {sms_count}/{total_rows} = {sms_rate:.2f}%\")\n",
    "\n",
    "# Nombre avec Handicap\n",
    "handicap_count = (df['Handicap'] > 0).sum()\n",
    "handicap_rate = (handicap_count / total_rows) * 100\n",
    "print(f\"=> population Ayant un handicap : {handicap_count}/{total_rows} = {handicap_rate:.2f}%\")\n",
    "\n",
    "# Nombre avec Alcoholism\n",
    "alcoholism_count = (df['Alcoholism'] == 1).sum()\n",
    "alcoholism_rate = (alcoholism_count / total_rows) * 100\n",
    "print(f\"=> population Ayant un alcoolisme : {alcoholism_count}/{total_rows} = {alcoholism_rate:.2f}%\")\n",
    "\n",
    "# Nombre avec Diabetes\n",
    "diabetes_count = (df['Diabetes'] == 1).sum()\n",
    "diabetes_rate = (diabetes_count / total_rows) * 100\n",
    "print(f\"=> population Ayant un diabète : {diabetes_count}/{total_rows} = {diabetes_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f6951",
   "metadata": {},
   "source": [
    "## STRATEGIE / 3° Pré - features engineering\n",
    "\n",
    "\n",
    "### Colonnes du dataset à garder : \n",
    "\n",
    "- Gender => M or F\t\n",
    "- Age => int\n",
    "- Neighbourhood => string\t\n",
    "- Scholarship\t\n",
    "- Hypertension\t\n",
    "- Diabetes\t\n",
    "- Alcoholism\t\n",
    "- Handicap\t\n",
    "- SMS_received\t\n",
    "- No-show\n",
    "\n",
    "### Nouvelles colonnes à créer :\n",
    "\n",
    "- \"DaysUntilAppointement\" => calcul en jours (approx) de AppointmentDay - ScheduledDay\n",
    "\n",
    "\n",
    "### Colonnes du dataset à virer :\n",
    "\n",
    "- PatientId\t\n",
    "- AppointmentID\t\n",
    "- ScheduledDay (après avoir créé DaysUntilAppointement)\t\n",
    "- AppointmentDay (après avoir créé DaysUntilAppointement)\n",
    "\n",
    "=> Le label est \"No-show\"\n",
    "\n",
    "\n",
    "\n",
    "### Voir si pas de cellules abbérantes\n",
    "\n",
    "- No show => Yes/No only\n",
    "- Age : pas de négatif et pas de + de 115 ans et que des integers \n",
    "- Scholarship\t/ Hypertension / Diabetes / Alcoholism\t/ Handicap\t/ SMS_received / No-show : que de 0 ou 1\n",
    "- Gender: ue des M ou F           \n",
    "- Neighbourhood doit être une string \n",
    "- AppointmentDay et ScheduledDay doivent être du type : 18/05/2016  09:18:03\n",
    "- 'Handicap' ne contient que les valeurs [0 1 2 3 4]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f660790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONTRÔLE DE VALEURS ABERRANTES POUR LA TARGET NO-SHOW ===\n",
      "Valeurs uniques dans 'No-show': ['No' 'Yes']\n",
      " Contrôle OK : La colonne 'No-show' ne contient que 'Yes' et 'No'\n"
     ]
    }
   ],
   "source": [
    "# Contrôle de Y\n",
    "print(\"\\n=== CONTRÔLE DE VALEURS ABERRANTES POUR LA TARGET NO-SHOW ===\")\n",
    "\n",
    "# Vérification des valeurs uniques dans No-show\n",
    "unique_values = df['No-show'].unique()\n",
    "print(f\"Valeurs uniques dans 'No-show': {unique_values}\")\n",
    "print(\" Contrôle OK : La colonne 'No-show' ne contient que 'Yes' et 'No'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc137b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONTRÔLE DE VALEURS ABERRANTES DE X ===\n",
      "\n",
      "=> Contrôle Scholarship', 'Hypertension', 'Diabetes', 'Alcoholism', 'SMS_received:\n",
      " Contrôle OK : La colonne 'Scholarship' ne contient que des valeurs binaires (0/1)\n",
      " Contrôle OK : La colonne 'Hypertension' ne contient que des valeurs binaires (0/1)\n",
      " Contrôle OK : La colonne 'Diabetes' ne contient que des valeurs binaires (0/1)\n",
      " Contrôle OK : La colonne 'Alcoholism' ne contient que des valeurs binaires (0/1)\n",
      " Contrôle OK : La colonne 'SMS_received' ne contient que des valeurs binaires (0/1)\n",
      "\n",
      "=> Contrôle Handicap:\n",
      " Contrôle OK : La colonne 'Handicap' contient uniquement des valeurs attendues: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "\n",
      "=> Contrôle Gender:\n",
      " Contrôle OK : La colonne 'Gender' ne contient que 'M' et 'F'\n",
      "\n",
      "=> Contrôle Neighbourhood:\n",
      "  Nombre de quartiers uniques: 81\n",
      "  Echantillon de 20 quartiers: ['AEROPORTO', 'ANDORINHAS', 'ANTÔNIO HONÓRIO', 'ARIOVALDO FAVALESSA', 'BARRO VERMELHO', 'BELA VISTA', 'BENTO FERREIRA', 'BOA VISTA', 'BONFIM', 'CARATOÍRA', 'CENTRO', 'COMDUSA', 'CONQUISTA', 'CONSOLAÇÃO', 'CRUZAMENTO', 'DA PENHA', 'DE LOURDES', 'DO CABRAL', 'DO MOSCOSO', 'DO QUADRO']\n",
      " Contrôle OK : La colonne 'Neighbourhood' est de type string avec 81 quartiers uniques\n",
      "\n",
      "=> Contrôle des dates:\n",
      "  Échantillon de 'AppointmentDay': 2016-04-29T00:00:00Z\n",
      "  'AppointmentDay' est de type string/object\n",
      " Toutes les valeurs de 'AppointmentDay' respectent le format ISO 8601 (YYYY-MM-DDTHH:MM:SSZ)\n",
      "  Échantillon de 'ScheduledDay': 2016-04-29T18:38:08Z\n",
      "  'ScheduledDay' est de type string/object\n",
      " Toutes les valeurs de 'ScheduledDay' respectent le format ISO 8601 (YYYY-MM-DDTHH:MM:SSZ)\n",
      "\n",
      "=> Contrôle de la colonne 'Age':\n",
      "  Type: int64, Min: -1, Max: 115\n",
      " Problème : 1 valeurs négatives détectées dans 'Age'\n",
      "\n",
      "=== NETTOYAGE DES DONNÉES ABERRANTES ===\n",
      " 1 ligne(s) avec Age négatif supprimée(s)\n",
      "  Nombre de lignes avant: 110527\n",
      "  Nombre de lignes après: 110526\n",
      " Pas de valeurs > 115 ans dans 'Age'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CONTRÔLE DE VALEURS ABERRANTES DE X ===\")\n",
    "# on degage les colonnes PatientId et AppointmentID\n",
    "relevantColumns = ['No-show', 'Scholarship', 'Gender', 'Age', 'Hypertension', 'Diabetes', 'Alcoholism', 'Handicap', 'SMS_received', 'ScheduledDay', 'AppointmentDay', 'Neighbourhood']\n",
    "preSelectedDf = df[relevantColumns].copy()\n",
    "\n",
    "# Contrôle des colonnes numériques binaires (0/1)\n",
    "print(\"\\n=> Contrôle Scholarship', 'Hypertension', 'Diabetes', 'Alcoholism', 'SMS_received:\")\n",
    "binary_columns = ['Scholarship', 'Hypertension', 'Diabetes', 'Alcoholism', 'SMS_received']\n",
    "for col in binary_columns:\n",
    "    unique_vals = preSelectedDf[col].unique()\n",
    "    if set(unique_vals).issubset({0, 1}):\n",
    "        print(f\" Contrôle OK : La colonne '{col}' ne contient que des valeurs binaires (0/1)\")\n",
    "    else:\n",
    "        print(f\" Problème détecté : La colonne '{col}' contient des valeurs autres que 0 et 1: {unique_vals}\")  \n",
    "\n",
    "# Contrôle de la colonne 'Handicap'\n",
    "print(\"\\n=> Contrôle Handicap:\")\n",
    "handicap_vals = preSelectedDf['Handicap'].unique()\n",
    "expected_handicap = {0, 1, 2, 3, 4}\n",
    "if set(handicap_vals).issubset(expected_handicap):\n",
    "    print(f\" Contrôle OK : La colonne 'Handicap' contient uniquement des valeurs attendues: {sorted(handicap_vals)}\")\n",
    "else:\n",
    "    unexpected = set(handicap_vals) - expected_handicap\n",
    "    print(f\" Problème détecté : La colonne 'Handicap' contient des valeurs inattendues: {unexpected}\")\n",
    "\n",
    "# Contrôle de la colonne 'Gender'\n",
    "print(\"\\n=> Contrôle Gender:\")\n",
    "gender_vals = preSelectedDf['Gender'].unique()\n",
    "if set(gender_vals) == {'M', 'F'}:\n",
    "    print(f\" Contrôle OK : La colonne 'Gender' ne contient que 'M' et 'F'\")\n",
    "else:\n",
    "    print(f\" Problème détecté : La colonne 'Gender' contient des valeurs autres que M/F: {gender_vals}\")\n",
    "\n",
    "# Contrôle de la colonne 'Neighbourhood'\n",
    "print(\"\\n=> Contrôle Neighbourhood:\")\n",
    "\n",
    "# Afficher les valeurs uniques\n",
    "unique_neighbourhoods = preSelectedDf['Neighbourhood'].unique()\n",
    "nb_unique = preSelectedDf['Neighbourhood'].nunique()\n",
    "print(f\"  Nombre de quartiers uniques: {nb_unique}\")\n",
    "print(f\"  Echantillon de 20 quartiers: {sorted(unique_neighbourhoods)[:20]}\")\n",
    "\n",
    "neighbourhood_type = preSelectedDf['Neighbourhood'].dtype\n",
    "if neighbourhood_type == 'object' or neighbourhood_type == 'string':\n",
    "    nb_unique = preSelectedDf['Neighbourhood'].nunique()\n",
    "    print(f\" Contrôle OK : La colonne 'Neighbourhood' est de type string avec {nb_unique} quartiers uniques\")\n",
    "else:\n",
    "    print(f\" Problème détecté : La colonne 'Neighbourhood' n'est pas de type string: {neighbourhood_type}\")\n",
    "\n",
    "# Contrôle des colonnes 'AppointmentDay' et 'ScheduledDay'\n",
    "print(\"\\n=> Contrôle des dates:\")\n",
    "import re\n",
    "# Pattern pour le format ISO 8601: YYYY-MM-DDTHH:MM:SSZ\n",
    "date_pattern = r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$'\n",
    "\n",
    "for col in ['AppointmentDay', 'ScheduledDay']:\n",
    "    # Afficher un échantillon\n",
    "    sample_value = str(preSelectedDf[col].iloc[0])\n",
    "    print(f\"  Échantillon de '{col}': {sample_value}\")\n",
    "    \n",
    "    # Vérifier le type\n",
    "    if preSelectedDf[col].dtype == 'object':\n",
    "        print(f\"  '{col}' est de type string/object\")\n",
    "        \n",
    "        # Vérifier le format sur toutes les valeurs\n",
    "        format_ok = preSelectedDf[col].astype(str).str.match(date_pattern).all()\n",
    "        \n",
    "        if format_ok:\n",
    "            print(f\" Toutes les valeurs de '{col}' respectent le format ISO 8601 (YYYY-MM-DDTHH:MM:SSZ)\")\n",
    "        else:\n",
    "            # Compter combien ne respectent pas le format\n",
    "            format_issues = (~preSelectedDf[col].astype(str).str.match(date_pattern)).sum()\n",
    "            print(f\" Problème : {format_issues} valeurs ne respectent pas le format ISO 8601\")\n",
    "            # Afficher quelques exemples de valeurs problématiques\n",
    "            invalid_samples = preSelectedDf[~preSelectedDf[col].astype(str).str.match(date_pattern)][col].head(3)\n",
    "            print(f\"  Exemples de valeurs invalides: {list(invalid_samples)}\")\n",
    "    else:\n",
    "        print(f\"  '{col}' est de type: {preSelectedDf[col].dtype}\")\n",
    "\n",
    "# Contrôle 'Age': pas de négatif et pas de + de 115 ans et que des integers\n",
    "age_min = preSelectedDf['Age'].min()\n",
    "age_max = preSelectedDf['Age'].max()\n",
    "age_dtype = preSelectedDf['Age'].dtype\n",
    "has_negative = (preSelectedDf['Age'] < 0).any()\n",
    "has_too_old = (preSelectedDf['Age'] > 115).any()\n",
    "\n",
    "print(f\"\\n=> Contrôle de la colonne 'Age':\")\n",
    "print(f\"  Type: {age_dtype}, Min: {age_min}, Max: {age_max}\")\n",
    "\n",
    "if has_negative:\n",
    "    count_negative = (preSelectedDf['Age'] < 0).sum()\n",
    "    print(f\" Problème : {count_negative} valeurs négatives détectées dans 'Age'\")\n",
    "    print(\"\\n=== NETTOYAGE DES DONNÉES ABERRANTES ===\")\n",
    "    # Suppression des lignes avec âge négatif\n",
    "    nb_rows_before = len(preSelectedDf)\n",
    "    preSelectedDf = preSelectedDf[preSelectedDf['Age'] >= 0]\n",
    "    nb_rows_after = len(preSelectedDf)\n",
    "    nb_deleted = nb_rows_before - nb_rows_after\n",
    "\n",
    "    if nb_deleted > 0:\n",
    "        print(f\" {nb_deleted} ligne(s) avec Age négatif supprimée(s)\")\n",
    "        print(f\"  Nombre de lignes avant: {nb_rows_before}\")\n",
    "        print(f\"  Nombre de lignes après: {nb_rows_after}\")\n",
    "\n",
    "else:\n",
    "    print(f\" Pas de valeurs négatives dans 'Age'\")\n",
    "\n",
    "if has_too_old:\n",
    "    count_too_old = (preSelectedDf['Age'] > 115).sum()\n",
    "    print(f\" Problème : {count_too_old} valeurs > 115 ans détectées dans 'Age'\")\n",
    "else:\n",
    "    print(f\" Pas de valeurs > 115 ans dans 'Age'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d18794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CRÉATION DE LA NOUVELLE COLONNE 'DaysUntilAppointement' ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> Reset à zéro heures pour les 2 colonnes:\n",
      "\n",
      "=> Contrôle des AppointmentDay < ScheduledDay détectées (ANOMALIES):\n",
      " Problème : 5 ligne(s) où AppointmentDay < ScheduledDay détectées\n",
      "  Ces lignes seront supprimées\n",
      "\n",
      "  Exemples de dates aberrantes:\n",
      "    Scheduled: 2016-05-10 00:00:00+00:00 > Appointment: 2016-05-09 00:00:00+00:00\n",
      "    Scheduled: 2016-05-18 00:00:00+00:00 > Appointment: 2016-05-17 00:00:00+00:00\n",
      "    Scheduled: 2016-05-05 00:00:00+00:00 > Appointment: 2016-05-04 00:00:00+00:00\n",
      "\n",
      " 5 ligne(s) supprimée(s)\n",
      "  Lignes restantes: 110521\n",
      " Colonne 'DaysUntilAppointement' créée avec succès\n",
      "  Min: 0 jours\n",
      "  Max: 179 jours\n",
      "  Moyenne: 10.18 jours\n",
      "\n",
      "=== SUPPRESSION DES COLONNES 'AppointmentDay', 'ScheduledDay' ===\n",
      " Colonnes 'AppointmentDay' et 'ScheduledDay' supprimées\n",
      "  Colonnes restantes: ['No-show', 'Scholarship', 'Gender', 'Age', 'Hypertension', 'Diabetes', 'Alcoholism', 'Handicap', 'SMS_received', 'Neighbourhood', 'DaysUntilAppointement']\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CRÉATION DE LA NOUVELLE COLONNE 'DaysUntilAppointement' ===\")\n",
    "# Conversion des colonnes de dates en datetime\n",
    "preSelectedDf['AppointmentDay'] = pd.to_datetime(preSelectedDf['AppointmentDay'])\n",
    "preSelectedDf['ScheduledDay'] = pd.to_datetime(preSelectedDf['ScheduledDay'])\n",
    "\n",
    "# il y a un biais dans les dates car on dans 30% des cas on a ScheduledDay > AppointmentDay, ce qui est impossible.\n",
    "# et dans la plupart des cas, on a des rdv ayant lieu le même jour que la réservation : \n",
    "# 2016-04-29 18:38:08+00:00 > Appointment: 2016-04-29 00:00:00+00:00\n",
    "# ceci s'explique que les heures de la colonne AppointmentDay sont reset à 00h00, et celles de : ScheduledDay ne le sont pas\n",
    "# Il faut donc remettre ScheduledDay et AppointmentDayà niveau, et transformer tous ses datetimes en 00:00:00\n",
    "print(\"\\n=> Reset à zéro heures pour les 2 colonnes:\")\n",
    "preSelectedDf['ScheduledDay'] = preSelectedDf['ScheduledDay'].dt.normalize()\n",
    "preSelectedDf['AppointmentDay'] = preSelectedDf['AppointmentDay'].dt.normalize()\n",
    "\n",
    "# Contrôle des dates aberrantes (AppointmentDay < ScheduledDay)\n",
    "print(\"\\n=> Contrôle des AppointmentDay < ScheduledDay détectées (ANOMALIES):\")\n",
    "dates_aberrantes = preSelectedDf['AppointmentDay'] < preSelectedDf['ScheduledDay']\n",
    "nb_dates_aberrantes = dates_aberrantes.sum()\n",
    "\n",
    "if nb_dates_aberrantes > 0:\n",
    "    print(f\" Problème : {nb_dates_aberrantes} ligne(s) où AppointmentDay < ScheduledDay détectées\")\n",
    "    print(f\"  Ces lignes seront supprimées\")\n",
    "    \n",
    "    # Afficher quelques exemples\n",
    "    exemples = preSelectedDf[dates_aberrantes][['ScheduledDay', 'AppointmentDay']].head(3)\n",
    "    print(f\"\\n  Exemples de dates aberrantes:\")\n",
    "    for idx, row in exemples.iterrows():\n",
    "        print(f\"    Scheduled: {row['ScheduledDay']} > Appointment: {row['AppointmentDay']}\")\n",
    "    \n",
    "    # Suppression des lignes aberrantes\n",
    "    nb_rows_before = len(preSelectedDf)\n",
    "    preSelectedDf = preSelectedDf[~dates_aberrantes]\n",
    "    nb_rows_after = len(preSelectedDf)\n",
    "    print(f\"\\n {nb_rows_before - nb_rows_after} ligne(s) supprimée(s)\")\n",
    "    print(f\"  Lignes restantes: {nb_rows_after}\")\n",
    "else:\n",
    "    print(f\" Toutes les dates sont cohérentes (AppointmentDay >= ScheduledDay)\")\n",
    "\n",
    "# Calcul de la différence en jours (arrondi au supérieur)\n",
    "preSelectedDf['DaysUntilAppointement'] = (preSelectedDf['AppointmentDay'] - preSelectedDf['ScheduledDay']).dt.total_seconds() / (24 * 3600)\n",
    "preSelectedDf['DaysUntilAppointement'] = np.ceil(preSelectedDf['DaysUntilAppointement']).astype(int)\n",
    "\n",
    "print(f\" Colonne 'DaysUntilAppointement' créée avec succès\")\n",
    "print(f\"  Min: {preSelectedDf['DaysUntilAppointement'].min()} jours\")\n",
    "print(f\"  Max: {preSelectedDf['DaysUntilAppointement'].max()} jours\")\n",
    "print(f\"  Moyenne: {preSelectedDf['DaysUntilAppointement'].mean():.2f} jours\")\n",
    "\n",
    "# On supprime les colonnes de dates maintenant qu'on a extrait l'information\n",
    "print(\"\\n=== SUPPRESSION DES COLONNES 'AppointmentDay', 'ScheduledDay' ===\")\n",
    "preSelectedDf = preSelectedDf.drop(columns=['AppointmentDay', 'ScheduledDay'])\n",
    "print(f\" Colonnes 'AppointmentDay' et 'ScheduledDay' supprimées\")\n",
    "print(f\"  Colonnes restantes: {list(preSelectedDf.columns)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f81c019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CRÉATION DE LA COLONNE 'nb_pathologies' ===\n",
      " Colonne 'nb_pathologies' créée avec succès\n",
      "  Valeurs possibles: [np.int64(0), np.int64(1), np.int64(2), np.int64(3)]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Création de la colonne nb_pathologies\n",
    "print(\"\\n=== CRÉATION DE LA COLONNE 'nb_pathologies' ===\")\n",
    "preSelectedDf['nb_pathologies'] = (preSelectedDf['Hypertension'] + \n",
    "                                    preSelectedDf['Diabetes'] + \n",
    "                                    preSelectedDf['Alcoholism'])\n",
    "print(f\" Colonne 'nb_pathologies' créée avec succès\")\n",
    "print(f\"  Valeurs possibles: {sorted(preSelectedDf['nb_pathologies'].unique())}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5505ad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUPPRESSION DE LA COLONNE 'SMS_received' ===\n",
      " Colonne 'SMS_received' supprimée\n",
      "  Colonnes restantes: ['No-show', 'Scholarship', 'Gender', 'Age', 'Hypertension', 'Diabetes', 'Alcoholism', 'Handicap', 'Neighbourhood', 'DaysUntilAppointement', 'nb_pathologies']\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# SUPPRESSION DE LA COLONNE SMS_received car valeur biaisée\n",
    "print(\"\\n=== SUPPRESSION DE LA COLONNE 'SMS_received' ===\")\n",
    "preSelectedDf = preSelectedDf.drop(columns=['SMS_received'])\n",
    "print(f\" Colonne 'SMS_received' supprimée\")\n",
    "print(f\"  Colonnes restantes: {list(preSelectedDf.columns)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab5957",
   "metadata": {},
   "source": [
    "### (NEW!) Transformation de Neighbourhood No-show en numériques\n",
    "\n",
    "- On utilise LabelEncoder pour transformer les noms de quartiers en entiers\n",
    "- On transforme les valeurs Yes/No en 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78d738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ENCODAGE DE 'Neighbourhood' EN INTEGERS ===\n",
      " Colonne 'Neighbourhood' encodée en integers\n",
      "  Nombre de quartiers uniques: 81\n",
      "  Valeurs min/max: 0 / 80\n",
      "  Type: int64\n",
      "\n",
      "  Exemples de mapping (5 premiers quartiers):\n",
      "    'AEROPORTO' -> 0\n",
      "    'ANDORINHAS' -> 1\n",
      "    'ANTÔNIO HONÓRIO' -> 2\n",
      "    'ARIOVALDO FAVALESSA' -> 3\n",
      "    'BARRO VERMELHO' -> 4\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== ENCODAGE DE 'Neighbourhood' EN INTEGERS ===\")\n",
    "# Créer un LabelEncoder pour Neighbourhood\n",
    "neighbourhood_encoder = LabelEncoder()\n",
    "preSelectedDf['Neighbourhood'] = neighbourhood_encoder.fit_transform(preSelectedDf['Neighbourhood'])\n",
    "\n",
    "print(f\" Colonne 'Neighbourhood' encodée en integers\")\n",
    "print(f\"  Nombre de quartiers uniques: {preSelectedDf['Neighbourhood'].nunique()}\")\n",
    "print(f\"  Valeurs min/max: {preSelectedDf['Neighbourhood'].min()} / {preSelectedDf['Neighbourhood'].max()}\")\n",
    "print(f\"  Type: {preSelectedDf['Neighbourhood'].dtype}\")\n",
    "\n",
    "# Afficher quelques exemples de mapping\n",
    "print(\"\\n  Exemples de mapping (5 premiers quartiers):\")\n",
    "for i, neighbourhood in enumerate(neighbourhood_encoder.classes_[:5]):\n",
    "    print(f\"    '{neighbourhood}' -> {i}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a23a76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRANSFORMATION DE 'No-show' EN 0/1 ===\n",
      " Colonne 'No-show' transformée en 0/1\n",
      "  Valeurs uniques: [np.int64(0), np.int64(1)]\n",
      "  Type: int64\n",
      "\n",
      "  Mapping: No -> 0, Yes -> 1\n",
      "  Taux de no-show (1): 20.19%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TRANSFORMATION DE 'No-show' EN 0/1 ===\")\n",
    "# Transformer Yes/No en 1/0\n",
    "preSelectedDf['No-show'] = (preSelectedDf['No-show'] == 'Yes').astype(int)\n",
    "\n",
    "print(f\" Colonne 'No-show' transformée en 0/1\")\n",
    "print(f\"  Valeurs uniques: {sorted(preSelectedDf['No-show'].unique())}\")\n",
    "print(f\"  Type: {preSelectedDf['No-show'].dtype}\")\n",
    "print(f\"\\n  Mapping: No -> 0, Yes -> 1\")\n",
    "print(f\"  Taux de no-show (1): {(preSelectedDf['No-show'] == 1).sum() / len(preSelectedDf) * 100:.2f}%\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b49fcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RÉSUMÉ DES DONNÉES PRÉPARÉES ===\n",
      "\n",
      "Nombre total de lignes: 110521\n",
      "\n",
      "Colonnes finales (11):\n",
      "  - No-show (int64)\n",
      "  - Scholarship (int64)\n",
      "  - Gender (object)\n",
      "  - Age (int64)\n",
      "  - Hypertension (int64)\n",
      "  - Diabetes (int64)\n",
      "  - Alcoholism (int64)\n",
      "  - Handicap (int64)\n",
      "  - Neighbourhood (int64)\n",
      "  - DaysUntilAppointement (int64)\n",
      "  - nb_pathologies (int64)\n",
      "\n",
      "Aperçu des premières lignes:\n",
      "   No-show  Scholarship Gender  Age  Hypertension  Diabetes  Alcoholism  \\\n",
      "0        0            0      F   62             1         0           0   \n",
      "1        0            0      M   56             0         0           0   \n",
      "2        0            0      F   62             0         0           0   \n",
      "3        0            0      F    8             0         0           0   \n",
      "4        0            0      F   56             1         1           0   \n",
      "\n",
      "   Handicap  Neighbourhood  DaysUntilAppointement  nb_pathologies  \n",
      "0         0             39                      0               1  \n",
      "1         0             39                      0               0  \n",
      "2         0             45                      0               0  \n",
      "3         0             54                      0               0  \n",
      "4         0             39                      0               2  \n",
      "\n",
      "================================================================================\n",
      " DONNÉES PRÊTES POUR L'ENTRAÎNEMENT\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Résumé final des données préparées\n",
    "print(\"\\n=== RÉSUMÉ DES DONNÉES PRÉPARÉES ===\")\n",
    "print(f\"\\nNombre total de lignes: {len(preSelectedDf)}\")\n",
    "print(f\"\\nColonnes finales ({len(preSelectedDf.columns)}):\")\n",
    "for col in preSelectedDf.columns:\n",
    "    print(f\"  - {col} ({preSelectedDf[col].dtype})\")\n",
    "\n",
    "print(f\"\\nAperçu des premières lignes:\")\n",
    "print(preSelectedDf.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" DONNÉES PRÊTES POUR L'ENTRAÎNEMENT\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6371d7",
   "metadata": {},
   "source": [
    "## (NEW!) ANALYSE DE CORRÉLATION AVEC LE LABEL\n",
    "\n",
    "Avant de sélectionner les features, analysons leur corrélation avec `No-show` (label) pour identifier les features peu pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da1f9446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANALYSE DE CORRÉLATION AVEC LE LABEL 'No-show' ===\n",
      "================================================================================\n",
      "\n",
      " Corrélation de chaque feature avec No-show (1 = absent, 0 = présent):\n",
      "\n",
      "\n",
      "  DaysUntilAppointement     : +0.1863    FORTE\n",
      "  Scholarship               : +0.0292    FAIBLE\n",
      "  Alcoholism                : -0.0002    TRÈS FAIBLE\n",
      "  Gender                    : -0.0041    TRÈS FAIBLE\n",
      "  Handicap                  : -0.0063    TRÈS FAIBLE\n",
      "  Neighbourhood             : -0.0090    TRÈS FAIBLE\n",
      "  Diabetes                  : -0.0152    TRÈS FAIBLE\n",
      "  nb_pathologies            : -0.0303    FAIBLE\n",
      "  Hypertension              : -0.0357    FAIBLE\n",
      "  Age                       : -0.0603    MOYENNE\n",
      "\n",
      " INTERPRÉTATION:\n",
      "  - Corrélation > 0.10 ou < -0.10 : Feature PERTINENTE\n",
      "  - Corrélation entre -0.02 et +0.02 : Feature PEU PERTINENTE (peut être retirée)\n",
      "  - Corrélation positive : Plus la valeur augmente, plus le risque d'absence augmente\n",
      "  - Corrélation négative : Plus la valeur augmente, moins le risque d'absence\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyse de corrélation entre les features et le label No-show\n",
    "print(\"\\n=== ANALYSE DE CORRÉLATION AVEC LE LABEL 'No-show' ===\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Encoder temporairement Gender pour le calcul de corrélation\n",
    "df_for_corr = preSelectedDf.copy()\n",
    "if df_for_corr['Gender'].dtype == 'object':\n",
    "    df_for_corr['Gender'] = (df_for_corr['Gender'] == 'M').astype(int)  # M=1, F=0\n",
    "\n",
    "# Calculer la corrélation de chaque feature avec No-show\n",
    "correlations = df_for_corr.corr()['No-show'].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n Corrélation de chaque feature avec No-show (1 = absent, 0 = présent):\")\n",
    "print(f\"{'' * 80}\\n\")\n",
    "\n",
    "for feature, corr in correlations.items():\n",
    "    if feature != 'No-show':\n",
    "        # Déterminer la force de la corrélation\n",
    "        abs_corr = abs(corr)\n",
    "        if abs_corr > 0.10:\n",
    "            strength = \"FORTE\"\n",
    "        elif abs_corr > 0.05:\n",
    "            strength = \"MOYENNE\"\n",
    "        elif abs_corr > 0.02:\n",
    "            strength = \"FAIBLE\"\n",
    "        else:\n",
    "            strength = \"TRÈS FAIBLE\"\n",
    "        \n",
    "        bar = '' * int(abs_corr * 100)\n",
    "        print(f\"  {feature:25s} : {corr:+.4f}  {bar}  {strength}\")\n",
    "\n",
    "print(\"\\n INTERPRÉTATION:\")\n",
    "print(\"  - Corrélation > 0.10 ou < -0.10 : Feature PERTINENTE\")\n",
    "print(\"  - Corrélation entre -0.02 et +0.02 : Feature PEU PERTINENTE (peut être retirée)\")\n",
    "print(\"  - Corrélation positive : Plus la valeur augmente, plus le risque d'absence augmente\")\n",
    "print(\"  - Corrélation négative : Plus la valeur augmente, moins le risque d'absence\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cb657",
   "metadata": {},
   "source": [
    "## NETTOYAGE DES FEATURES\n",
    "\n",
    "Basé sur l'analyse de corrélation, nous allons:\n",
    "1. **Retirer les features redondantes** : `Diabetes`, `Hypertension`, `Alcoholism` (car incluses dans `nb_pathologies`)\n",
    "2. **Retirer les features peu corrélées** : Celles avec corrélation absolue < 0.02 avec le label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c0138a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NETTOYAGE DES FEATURES ===\n",
      "================================================================================\n",
      "\n",
      " Features à retirer (6):\n",
      "\n",
      "   Alcoholism                : REDONDANTE (incluse dans nb_pathologies) + PEU CORRÉLÉE (-0.0002)\n",
      "   Diabetes                  : REDONDANTE (incluse dans nb_pathologies) + PEU CORRÉLÉE (-0.0152)\n",
      "   Gender                    : PEU CORRÉLÉE (-0.0041)\n",
      "   Handicap                  : PEU CORRÉLÉE (-0.0063)\n",
      "   Hypertension              : REDONDANTE (incluse dans nb_pathologies)\n",
      "   Neighbourhood             : PEU CORRÉLÉE (-0.0090)\n",
      "\n",
      "⏳ Suppression des features non pertinentes...\n",
      " 6 feature(s) supprimée(s)\n",
      "  Features avant: 11 → Features après: 5\n",
      "\n",
      " Features finales conservées (4 features + 1 label):\n",
      "\n",
      "   Age                       (corrélation: -0.0603)\n",
      "   DaysUntilAppointement     (corrélation: +0.1863)\n",
      "   Scholarship               (corrélation: +0.0292)\n",
      "   nb_pathologies            (corrélation: -0.0303)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Identification et suppression des features non pertinentes\n",
    "print(\"\\n=== NETTOYAGE DES FEATURES ===\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Features redondantes (incluses dans nb_pathologies)\n",
    "redundant_features = ['Diabetes', 'Hypertension', 'Alcoholism']\n",
    "\n",
    "# 2. Features peu corrélées avec le label (|corrélation| < 0.02)\n",
    "low_corr_features = []\n",
    "for feature, corr in correlations.items():\n",
    "    if feature != 'No-show' and abs(corr) < 0.02:\n",
    "        low_corr_features.append(feature)\n",
    "\n",
    "# Combiner les deux listes\n",
    "features_to_remove = list(set(redundant_features + low_corr_features))\n",
    "\n",
    "print(f\"\\n Features à retirer ({len(features_to_remove)}):\")\n",
    "print(f\"{'' * 80}\")\n",
    "\n",
    "if features_to_remove:\n",
    "    for feat in sorted(features_to_remove):\n",
    "        reason = \"\"\n",
    "        if feat in redundant_features:\n",
    "            reason = \"REDONDANTE (incluse dans nb_pathologies)\"\n",
    "        if feat in low_corr_features:\n",
    "            corr_val = correlations[feat]\n",
    "            reason += f\" + PEU CORRÉLÉE ({corr_val:+.4f})\" if reason else f\"PEU CORRÉLÉE ({corr_val:+.4f})\"\n",
    "        print(f\"   {feat:25s} : {reason}\")\n",
    "    \n",
    "    # Suppression des features\n",
    "    print(f\"\\n⏳ Suppression des features non pertinentes...\")\n",
    "    features_before = list(preSelectedDf.columns)\n",
    "    preSelectedDf = preSelectedDf.drop(columns=features_to_remove)\n",
    "    features_after = list(preSelectedDf.columns)\n",
    "    \n",
    "    print(f\" {len(features_to_remove)} feature(s) supprimée(s)\")\n",
    "    print(f\"  Features avant: {len(features_before)} → Features après: {len(features_after)}\")\n",
    "else:\n",
    "    print(\"  ℹ Aucune feature à retirer (toutes sont pertinentes)\")\n",
    "\n",
    "print(f\"\\n Features finales conservées ({len(preSelectedDf.columns) - 1} features + 1 label):\")\n",
    "print(f\"{'' * 80}\")\n",
    "for col in sorted(preSelectedDf.columns):\n",
    "    if col != 'No-show':\n",
    "        corr_val = correlations[col] if col in correlations else 0\n",
    "        print(f\"   {col:25s} (corrélation: {corr_val:+.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3988a",
   "metadata": {},
   "source": [
    "## MODÉLISATION : GRADIENT BOOSTING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "919cfa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PRÉPARATION DES DONNÉES POUR L'ENTRAÎNEMENT ===\n",
      " Target (y) : No-show\n",
      "  - Forme: (110521,)\n",
      "  - Classe 0 (Présents): 88207 (79.81%)\n",
      "  - Classe 1 (Absents): 22314 (20.19%)\n",
      "\n",
      " Features (X) : 4 colonnes\n",
      "  - Scholarship\n",
      "  - Age\n",
      "  - DaysUntilAppointement\n",
      "  - nb_pathologies\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Séparation des features (X) et de la target (y)\n",
    "print(\"\\n=== PRÉPARATION DES DONNÉES POUR L'ENTRAÎNEMENT ===\")\n",
    "\n",
    "# Target\n",
    "y = preSelectedDf['No-show']\n",
    "\n",
    "# Features (tout sauf No-show)\n",
    "X = preSelectedDf.drop(columns=['No-show'])\n",
    "\n",
    "print(f\" Target (y) : {y.name}\")\n",
    "print(f\"  - Forme: {y.shape}\")\n",
    "print(f\"  - Classe 0 (Présents): {(y == 0).sum()} ({(y == 0).sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"  - Classe 1 (Absents): {(y == 1).sum()} ({(y == 1).sum()/len(y)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n Features (X) : {len(X.columns)} colonnes\")\n",
    "for col in X.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61618de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DÉFINITION DES TYPES DE FEATURES ===\n",
      " Features catégorielles (2): ['Scholarship', 'nb_pathologies']\n",
      " Features numériques (2): ['Age', 'DaysUntilAppointement']\n",
      "\n",
      " Note: Diabetes, Hypertension, Alcoholism ont été retirées (redondantes avec nb_pathologies)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Définition des features catégorielles et numériques\n",
    "print(\"\\n=== DÉFINITION DES TYPES DE FEATURES ===\")\n",
    "\n",
    "# Récupérer automatiquement les features restantes après nettoyage\n",
    "all_features = [col for col in preSelectedDf.columns if col != 'No-show']\n",
    "\n",
    "# Features catégorielles (types object ou colonnes binaires/discrètes)\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "\n",
    "for col in all_features:\n",
    "    if preSelectedDf[col].dtype in ['object', 'category'] or preSelectedDf[col].nunique() <= 10:\n",
    "        categorical_features.append(col)\n",
    "    else:\n",
    "        numerical_features.append(col)\n",
    "\n",
    "print(f\" Features catégorielles ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\" Features numériques ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"\\n Note: Diabetes, Hypertension, Alcoholism ont été retirées (redondantes avec nb_pathologies)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2697f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CRÉATION DU PIPELINE DE PREPROCESSING ===\n",
      " Pipeline de preprocessing créé:\n",
      "  - Features catégorielles → OrdinalEncoder\n",
      "  - Features numériques → StandardScaler\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Création des transformers pour le preprocessing\n",
    "print(\"\\n=== CRÉATION DU PIPELINE DE PREPROCESSING ===\")\n",
    "\n",
    "# Transformer pour les features catégorielles\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# Transformer pour les features numériques\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# ColumnTransformer pour appliquer les transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ])\n",
    "\n",
    "print(\" Pipeline de preprocessing créé:\")\n",
    "print(\"  - Features catégorielles → OrdinalEncoder\")\n",
    "print(\"  - Features numériques → StandardScaler\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bfe44c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SPLIT TRAIN/TEST ===\n",
      " Split effectué avec stratification\n",
      "\n",
      "  Train set: 88416 échantillons\n",
      "    - Classe 0 (Présents): 70565 (79.81%)\n",
      "    - Classe 1 (Absents): 17851 (20.19%)\n",
      "\n",
      "  Test set: 22105 échantillons\n",
      "    - Classe 0 (Présents): 17642 (79.81%)\n",
      "    - Classe 1 (Absents): 4463 (20.19%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Split train/test avec stratification\n",
    "print(\"\\n=== SPLIT TRAIN/TEST ===\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Important : maintenir la proportion 80/20 dans train et test\n",
    ")\n",
    "\n",
    "print(f\" Split effectué avec stratification\")\n",
    "print(f\"\\n  Train set: {len(X_train)} échantillons\")\n",
    "print(f\"    - Classe 0 (Présents): {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"    - Classe 1 (Absents): {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Test set: {len(X_test)} échantillons\")\n",
    "print(f\"    - Classe 0 (Présents): {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"    - Classe 1 (Absents): {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e614102",
   "metadata": {},
   "source": [
    "## OPTIMISATION DES HYPERPARAMÈTRES PAR GRIDSEARCHCV\n",
    "\n",
    "Avant d'entraîner le modèle, nous allons utiliser **GridSearchCV** pour trouver les meilleurs hyperparamètres du Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e745cf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OPTIMISATION DES HYPERPARAMÈTRES (GRIDSEARCHCV) ===\n",
      "================================================================================\n",
      "\n",
      " Grille de recherche configurée:\n",
      "  - n_estimators: [200, 300]\n",
      "  - learning_rate: [0.05, 0.1]\n",
      "  - max_depth: [5, 7]\n",
      "  - min_samples_split: [10, 20]\n",
      "  - min_samples_leaf: [5]\n",
      "  - subsample: [0.8]\n",
      "\n",
      "  Total de combinaisons: 16\n",
      "  Cross-validation: 3 folds\n",
      "  Total d'entraînements: 48 modèles\n",
      "\n",
      "⏳ Lancement de la recherche GridSearch...\n",
      "  Métrique d'optimisation: F1-Score (classe 1 - absents)\n",
      "    Ceci peut prendre plusieurs minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  13.9s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  14.3s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  14.2s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  16.1s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  19.9s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  24.4s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  26.5s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  13.6s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  27.3s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  21.5s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  21.9s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  25.6s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  23.8s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  20.0s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  26.7s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  30.9s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  31.9s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  25.7s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  36.5s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  29.5s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  29.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  15.6s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  31.4s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  16.2s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  33.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  16.8s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  16.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  27.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  25.3s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  50.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  16.8s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  37.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  45.8s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  25.8s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  26.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  24.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  23.2s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  36.0s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  47.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  46.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  52.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  36.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  23.0s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=200, classifier__subsample=0.8; total time=  20.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  47.2s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  22.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  20.2s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__min_samples_leaf=5, classifier__min_samples_split=20, classifier__n_estimators=300, classifier__subsample=0.8; total time=  22.8s\n",
      "\n",
      " GridSearch terminé en 187.37 secondes (3.1 minutes)\n",
      "\n",
      " MEILLEURS HYPERPARAMÈTRES TROUVÉS:\n",
      "\n",
      "  learning_rate            : 0.1\n",
      "  max_depth                : 7\n",
      "  min_samples_leaf         : 5\n",
      "  min_samples_split        : 10\n",
      "  n_estimators             : 300\n",
      "  subsample                : 0.8\n",
      "\n",
      " Meilleur F1-Score (CV): 0.1134\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration du GridSearchCV pour trouver les meilleurs hyperparamètres\n",
    "print(\"\\n=== OPTIMISATION DES HYPERPARAMÈTRES (GRIDSEARCHCV) ===\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Création d'un modèle de base pour GridSearch\n",
    "gb_base = GradientBoostingClassifier(random_state=42, verbose=0)\n",
    "\n",
    "# Pipeline pour GridSearch\n",
    "pipeline_search = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', gb_base)\n",
    "])\n",
    "\n",
    "# Grille de paramètres à tester\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [200, 300],\n",
    "    'classifier__learning_rate': [0.05, 0.1],\n",
    "    'classifier__max_depth': [5, 7],\n",
    "    'classifier__min_samples_split': [10, 20],\n",
    "    'classifier__min_samples_leaf': [5],\n",
    "    'classifier__subsample': [0.8]\n",
    "}\n",
    "\n",
    "# Calcul du nombre de combinaisons\n",
    "n_combinations = 1\n",
    "for param_values in param_grid.values():\n",
    "    n_combinations *= len(param_values)\n",
    "\n",
    "print(f\"\\n Grille de recherche configurée:\")\n",
    "print(f\"  - n_estimators: {param_grid['classifier__n_estimators']}\")\n",
    "print(f\"  - learning_rate: {param_grid['classifier__learning_rate']}\")\n",
    "print(f\"  - max_depth: {param_grid['classifier__max_depth']}\")\n",
    "print(f\"  - min_samples_split: {param_grid['classifier__min_samples_split']}\")\n",
    "print(f\"  - min_samples_leaf: {param_grid['classifier__min_samples_leaf']}\")\n",
    "print(f\"  - subsample: {param_grid['classifier__subsample']}\")\n",
    "print(f\"\\n  Total de combinaisons: {n_combinations}\")\n",
    "print(f\"  Cross-validation: 3 folds\")\n",
    "print(f\"  Total d'entraînements: {n_combinations * 3} modèles\")\n",
    "\n",
    "# Configuration de GridSearchCV avec F1-Score comme métrique d'optimisation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_search,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='f1',  # Optimiser le F1-Score (classe 1 par défaut car c'est la positive)\n",
    "    n_jobs=-1,  # Utiliser tous les CPU disponibles\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(f\"\\n⏳ Lancement de la recherche GridSearch...\")\n",
    "print(f\"  Métrique d'optimisation: F1-Score (classe 1 - absents)\")\n",
    "print(f\"    Ceci peut prendre plusieurs minutes...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Entraînement GridSearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n GridSearch terminé en {elapsed_time:.2f} secondes ({elapsed_time/60:.1f} minutes)\")\n",
    "\n",
    "# Affichage des meilleurs paramètres\n",
    "print(f\"\\n MEILLEURS HYPERPARAMÈTRES TROUVÉS:\")\n",
    "print(f\"{'' * 80}\")\n",
    "best_params = grid_search.best_params_\n",
    "for param_name, param_value in best_params.items():\n",
    "    clean_name = param_name.replace('classifier__', '')\n",
    "    print(f\"  {clean_name:25s}: {param_value}\")\n",
    "\n",
    "print(f\"\\n Meilleur F1-Score (CV): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6970ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ENTRAÎNEMENT DU MODÈLE OPTIMISÉ ===\n",
      "================================================================================\n",
      " Meilleur modèle récupéré depuis GridSearch\n",
      "  Le modèle a déjà été entraîné avec les meilleurs hyperparamètres\n",
      "\n",
      " Configuration finale du modèle:\n",
      "  - n_estimators: 300\n",
      "  - learning_rate: 0.1\n",
      "  - max_depth: 7\n",
      "  - min_samples_split: 10\n",
      "  - min_samples_leaf: 5\n",
      "  - subsample: 0.8\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Récupération du meilleur modèle et entraînement final\n",
    "print(\"\\n=== ENTRAÎNEMENT DU MODÈLE OPTIMISÉ ===\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Le modèle est déjà entraîné via GridSearch sur X_train\n",
    "# On récupère le meilleur estimateur (pipeline complet)\n",
    "model_pipeline = grid_search.best_estimator_\n",
    "\n",
    "print(f\" Meilleur modèle récupéré depuis GridSearch\")\n",
    "print(f\"  Le modèle a déjà été entraîné avec les meilleurs hyperparamètres\")\n",
    "\n",
    "# Extraction des hyperparamètres du modèle optimisé pour affichage\n",
    "gb_optimized = model_pipeline.named_steps['classifier']\n",
    "print(f\"\\n Configuration finale du modèle:\")\n",
    "print(f\"  - n_estimators: {gb_optimized.n_estimators}\")\n",
    "print(f\"  - learning_rate: {gb_optimized.learning_rate}\")\n",
    "print(f\"  - max_depth: {gb_optimized.max_depth}\")\n",
    "print(f\"  - min_samples_split: {gb_optimized.min_samples_split}\")\n",
    "print(f\"  - min_samples_leaf: {gb_optimized.min_samples_leaf}\")\n",
    "print(f\"  - subsample: {gb_optimized.subsample}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8790a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PRÉDICTIONS ===\n",
      " Prédictions train: 88416 échantillons\n",
      " Prédictions test: 22105 échantillons\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Prédictions sur les ensembles train et test\n",
    "print(\"\\n=== PRÉDICTIONS ===\")\n",
    "\n",
    "y_pred_train = model_pipeline.predict(X_train)\n",
    "y_pred_test = model_pipeline.predict(X_test)\n",
    "\n",
    "# Probabilités de prédiction (utile pour comprendre la confiance du modèle)\n",
    "y_proba_test = model_pipeline.predict_proba(X_test)\n",
    "\n",
    "print(f\" Prédictions train: {len(y_pred_train)} échantillons\")\n",
    "print(f\" Prédictions test: {len(y_pred_test)} échantillons\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc370e",
   "metadata": {},
   "source": [
    "## ÉVALUATION DU MODÈLE OPTIMISÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "774b29a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " ÉVALUATION COMPLÈTE DU MODÈLE OPTIMISÉ\n",
      "================================================================================\n",
      "\n",
      "### 1⃣ ACCURACY (Exactitude globale)\n",
      "\n",
      "  Train : 0.8115 (81.15%)\n",
      "  Test  : 0.7917 (79.17%)\n",
      "\n",
      "   Interprétation: Sur 100 prédictions, 79.2 sont correctes\n",
      "\n",
      "### 2⃣ MATRICE DE CONFUSION (Test)\n",
      "\n",
      "\n",
      "                      Prédit: Présent (0)    Prédit: Absent (1)\n",
      "  Réel: Présent (0)          17259                    383     \n",
      "  Réel: Absent  (1)           4221                    242     \n",
      "\n",
      "   Vrais Négatifs (VN): 17259 - Prédits présents et sont venus\n",
      "   Vrais Positifs (VP): 242 - Prédits absents et ne sont pas venus\n",
      "   Faux Positifs (FP): 383 - Prédits absents mais sont venus (FAUSSE ALERTE)\n",
      "   Faux Négatifs (FN): 4221 - Prédits présents mais ne sont pas venus (MANQUÉ)\n",
      "\n",
      "### 3⃣ MÉTRIQUES DÉTAILLÉES PAR CLASSE\n",
      "\n",
      "\n",
      " CLASSE 0 (Présents - No-show=0)\n",
      "  Precision : 0.8035 (80.35%)\n",
      "  Recall    : 0.9783 (97.83%)\n",
      "  F1-Score  : 0.8823\n",
      "\n",
      " CLASSE 1 (Absents - No-show=1)  PRIORITAIRE\n",
      "  Precision : 0.3872 (38.72%)\n",
      "    → Sur 100 prédits 'absents', 39 le sont vraiment\n",
      "    → Fausses alertes: 61%\n",
      "  Recall    : 0.0542 (5.42%)\n",
      "    → Sur 100 absents réels, on en détecte 5\n",
      "    → Absents manqués: 95%\n",
      "  F1-Score  : 0.0951\n",
      "\n",
      "### 4⃣ RAPPORT DE CLASSIFICATION COMPLET (Test)\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Classe 0: Présent       0.80      0.98      0.88     17642\n",
      " Classe 1: Absent       0.39      0.05      0.10      4463\n",
      "\n",
      "         accuracy                           0.79     22105\n",
      "        macro avg       0.60      0.52      0.49     22105\n",
      "     weighted avg       0.72      0.79      0.72     22105\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ÉVALUATION COMPLÈTE DU MODÈLE OPTIMISÉ\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" ÉVALUATION COMPLÈTE DU MODÈLE OPTIMISÉ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Accuracy\n",
    "train_accuracy_opt = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy_opt = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\n### 1⃣ ACCURACY (Exactitude globale)\")\n",
    "print(f\"{'' * 80}\")\n",
    "print(f\"  Train : {train_accuracy_opt:.4f} ({train_accuracy_opt*100:.2f}%)\")\n",
    "print(f\"  Test  : {test_accuracy_opt:.4f} ({test_accuracy_opt*100:.2f}%)\")\n",
    "print(f\"\\n   Interprétation: Sur 100 prédictions, {test_accuracy_opt*100:.1f} sont correctes\")\n",
    "\n",
    "# Matrice de confusion\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "vn, fp, fn, vp = cm_test.ravel()\n",
    "\n",
    "print(\"\\n### 2⃣ MATRICE DE CONFUSION (Test)\")\n",
    "print(f\"{'' * 80}\")\n",
    "print(f\"\\n                      Prédit: Présent (0)    Prédit: Absent (1)\")\n",
    "print(f\"  Réel: Présent (0)         {vn:>6}                 {fp:>6}     \")\n",
    "print(f\"  Réel: Absent  (1)         {fn:>6}                 {vp:>6}     \")\n",
    "print(f\"\\n   Vrais Négatifs (VN): {vn} - Prédits présents et sont venus\")\n",
    "print(f\"   Vrais Positifs (VP): {vp} - Prédits absents et ne sont pas venus\")\n",
    "print(f\"   Faux Positifs (FP): {fp} - Prédits absents mais sont venus (FAUSSE ALERTE)\")\n",
    "print(f\"   Faux Négatifs (FN): {fn} - Prédits présents mais ne sont pas venus (MANQUÉ)\")\n",
    "\n",
    "# Métriques détaillées par classe\n",
    "print(\"\\n### 3⃣ MÉTRIQUES DÉTAILLÉES PAR CLASSE\")\n",
    "print(f\"{'' * 80}\")\n",
    "\n",
    "# Classe 0 (Présents)\n",
    "precision_0 = precision_score(y_test, y_pred_test, pos_label=0)\n",
    "recall_0 = recall_score(y_test, y_pred_test, pos_label=0)\n",
    "f1_0 = f1_score(y_test, y_pred_test, pos_label=0)\n",
    "\n",
    "print(f\"\\n CLASSE 0 (Présents - No-show=0)\")\n",
    "print(f\"  Precision : {precision_0:.4f} ({precision_0*100:.2f}%)\")\n",
    "print(f\"  Recall    : {recall_0:.4f} ({recall_0*100:.2f}%)\")\n",
    "print(f\"  F1-Score  : {f1_0:.4f}\")\n",
    "\n",
    "# Classe 1 (Absents) - LA PLUS IMPORTANTE\n",
    "precision_1_opt = precision_score(y_test, y_pred_test, pos_label=1)\n",
    "recall_1_opt = recall_score(y_test, y_pred_test, pos_label=1)\n",
    "f1_1_opt = f1_score(y_test, y_pred_test, pos_label=1)\n",
    "\n",
    "print(f\"\\n CLASSE 1 (Absents - No-show=1)  PRIORITAIRE\")\n",
    "print(f\"  Precision : {precision_1_opt:.4f} ({precision_1_opt*100:.2f}%)\")\n",
    "print(f\"    → Sur 100 prédits 'absents', {precision_1_opt*100:.0f} le sont vraiment\")\n",
    "print(f\"    → Fausses alertes: {(1-precision_1_opt)*100:.0f}%\")\n",
    "print(f\"  Recall    : {recall_1_opt:.4f} ({recall_1_opt*100:.2f}%)\")\n",
    "print(f\"    → Sur 100 absents réels, on en détecte {recall_1_opt*100:.0f}\")\n",
    "print(f\"    → Absents manqués: {(1-recall_1_opt)*100:.0f}%\")\n",
    "print(f\"  F1-Score  : {f1_1_opt:.4f}\")\n",
    "\n",
    "# Rapport de classification complet\n",
    "print(\"\\n### 4⃣ RAPPORT DE CLASSIFICATION COMPLET (Test)\")\n",
    "print(f\"{'' * 80}\")\n",
    "print(classification_report(y_test, y_pred_test, \n",
    "                          target_names=['Classe 0: Présent', 'Classe 1: Absent']))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d2d23",
   "metadata": {},
   "source": [
    "## AMÉLIORATION DU RECALL\n",
    "\n",
    "Le modèle optimisé a un Recall faible (~5-10%) sur la classe minoritaire (absents).\n",
    "Nous allons tester plusieurs techniques pour l'améliorer :\n",
    "\n",
    "1. **Ajustement du seuil de décision** (threshold tuning)\n",
    "2. **Rééquilibrage avec SMOTE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d1e079",
   "metadata": {},
   "source": [
    "### Ajustement du seuil de décision\n",
    "\n",
    "Au lieu d'utiliser le seuil par défaut de 0.5, nous allons tester différents seuils pour favoriser la détection des absents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "481b5c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AJUSTEMENT DU SEUIL DE DÉCISION ===\n",
      "================================================================================\n",
      "\n",
      " IMPACT DU SEUIL SUR LES MÉTRIQUES:\n",
      "\n",
      "\n",
      "Seuil    Precision    Recall       F1-Score     Accuracy    \n",
      "\n",
      "0.50     0.3872       0.0542       0.0951       0.7917      \n",
      "0.40     0.3735       0.1714       0.2350       0.7747      \n",
      "0.30     0.3416       0.4481       0.3877       0.7142      \n",
      "0.25     0.3228       0.6003       0.4199       0.6651      \n",
      "0.20     0.3058       0.7804       0.4394       0.5980      \n",
      "0.15     0.2899       0.8824       0.4364       0.5399      \n",
      "\n",
      "\n",
      " MEILLEUR SEUIL POUR RECALL:\n",
      "  Seuil optimal: 0.15\n",
      "  Precision: 0.2899\n",
      "  Recall: 0.8824\n",
      "  F1-Score: 0.4364\n",
      "  Accuracy: 0.5399\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test de différents seuils de décision\n",
    "print(\"\\n=== AJUSTEMENT DU SEUIL DE DÉCISION ===\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Obtenir les probabilités de prédiction\n",
    "y_proba_test_opt = model_pipeline.predict_proba(X_test)[:, 1]  # Probabilité classe 1 (absent)\n",
    "\n",
    "# Tester différents seuils\n",
    "thresholds = [0.5, 0.4, 0.3, 0.25, 0.2, 0.15]\n",
    "results_threshold = []\n",
    "\n",
    "print(\"\\n IMPACT DU SEUIL SUR LES MÉTRIQUES:\")\n",
    "print(f\"{'' * 80}\\n\")\n",
    "print(f\"{'Seuil':<8} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Accuracy':<12}\")\n",
    "print(f\"{'' * 80}\")\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Prédictions avec le nouveau seuil\n",
    "    y_pred_threshold = (y_proba_test_opt >= threshold).astype(int)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    precision = precision_score(y_test, y_pred_threshold, pos_label=1)\n",
    "    recall = recall_score(y_test, y_pred_threshold, pos_label=1)\n",
    "    f1 = f1_score(y_test, y_pred_threshold, pos_label=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "    \n",
    "    results_threshold.append({\n",
    "        'threshold': threshold,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "    \n",
    "    print(f\"{threshold:<8.2f} {precision:<12.4f} {recall:<12.4f} {f1:<12.4f} {accuracy:<12.4f}\")\n",
    "\n",
    "print(f\"{'' * 80}\")\n",
    "\n",
    "# Trouver le meilleur seuil pour Recall\n",
    "best_recall_idx = max(range(len(results_threshold)), key=lambda i: results_threshold[i]['recall'])\n",
    "best_recall_result = results_threshold[best_recall_idx]\n",
    "\n",
    "print(f\"\\n MEILLEUR SEUIL POUR RECALL:\")\n",
    "print(f\"  Seuil optimal: {best_recall_result['threshold']:.2f}\")\n",
    "print(f\"  Precision: {best_recall_result['precision']:.4f}\")\n",
    "print(f\"  Recall: {best_recall_result['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {best_recall_result['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {best_recall_result['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10a13a",
   "metadata": {},
   "source": [
    "### Rééquilibrage avec SMOTE\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) génère des exemples synthétiques de la classe minoritaire pour rééquilibrer le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecc501b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RÉÉQUILIBRAGE AVEC SMOTE ===\n",
      "================================================================================\n",
      "\n",
      " Distribution AVANT SMOTE:\n",
      "  Classe 0 (Présents): 70565 (79.81%)\n",
      "  Classe 1 (Absents): 17851 (20.19%)\n",
      "  Total: 88416\n",
      "\n",
      " Distribution APRÈS SMOTE:\n",
      "  Classe 0 (Présents): 70565 (66.67%)\n",
      "  Classe 1 (Absents): 35282 (33.33%)\n",
      "  Total: 105847 (augmentation de 17431 exemples)\n",
      "\n",
      "⏳ Entraînement du modèle avec données SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pthaochanta/CODE/PET/ai/ai-dev-alyra-complete-training/livrables/ml/venv/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement terminé en 18.48 secondes\n",
      "\n",
      " RÉSULTATS AVEC SMOTE:\n",
      "\n",
      "  Precision (classe 1): 0.3671 (36.71%)\n",
      "  Recall (classe 1):    0.2283 (22.83%)\n",
      "  F1-Score (classe 1):  0.2815\n",
      "  Accuracy:             0.7647 (76.47%)\n",
      "\n",
      " COMPARAISON SMOTE vs MODÈLE OPTIMISÉ:\n",
      "\n",
      "  Recall: 0.0542 → 0.2283 (gain: +17.41%)\n",
      "  F1-Score: 0.0951 → 0.2815 (gain: +18.64%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Application de SMOTE sur les données d'entraînement\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"\\n=== RÉÉQUILIBRAGE AVEC SMOTE ===\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Distribution avant SMOTE\n",
    "print(\"\\n Distribution AVANT SMOTE:\")\n",
    "print(f\"  Classe 0 (Présents): {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"  Classe 1 (Absents): {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"  Total: {len(y_train)}\")\n",
    "\n",
    "# Appliquer le preprocessing avant SMOTE\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Application de SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.5)  # Rééquilibre à 50% (1:2 au lieu de 1:4)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "# Distribution après SMOTE\n",
    "print(\"\\n Distribution APRÈS SMOTE:\")\n",
    "print(f\"  Classe 0 (Présents): {(y_train_smote == 0).sum()} ({(y_train_smote == 0).sum()/len(y_train_smote)*100:.2f}%)\")\n",
    "print(f\"  Classe 1 (Absents): {(y_train_smote == 1).sum()} ({(y_train_smote == 1).sum()/len(y_train_smote)*100:.2f}%)\")\n",
    "print(f\"  Total: {len(y_train_smote)} (augmentation de {len(y_train_smote) - len(y_train)} exemples)\")\n",
    "\n",
    "print(\"\\n⏳ Entraînement du modèle avec données SMOTE...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Créer et entraîner un nouveau modèle avec les meilleurs hyperparamètres\n",
    "# Extraire les paramètres sans le préfixe 'classifier__'\n",
    "gb_params = {k.replace('classifier__', ''): v for k, v in best_params.items()}\n",
    "gb_smote = GradientBoostingClassifier(**gb_params)\n",
    "gb_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Entraînement terminé en {elapsed_time:.2f} secondes\")\n",
    "\n",
    "# Prédictions\n",
    "y_pred_test_smote = gb_smote.predict(X_test_preprocessed)\n",
    "\n",
    "# Évaluation\n",
    "print(\"\\n RÉSULTATS AVEC SMOTE:\")\n",
    "print(f\"{'' * 80}\")\n",
    "\n",
    "precision_smote = precision_score(y_test, y_pred_test_smote, pos_label=1)\n",
    "recall_smote = recall_score(y_test, y_pred_test_smote, pos_label=1)\n",
    "f1_smote = f1_score(y_test, y_pred_test_smote, pos_label=1)\n",
    "accuracy_smote = accuracy_score(y_test, y_pred_test_smote)\n",
    "\n",
    "print(f\"  Precision (classe 1): {precision_smote:.4f} ({precision_smote*100:.2f}%)\")\n",
    "print(f\"  Recall (classe 1):    {recall_smote:.4f} ({recall_smote*100:.2f}%)\")\n",
    "print(f\"  F1-Score (classe 1):  {f1_smote:.4f}\")\n",
    "print(f\"  Accuracy:             {accuracy_smote:.4f} ({accuracy_smote*100:.2f}%)\")\n",
    "\n",
    "# Comparaison avec le modèle optimisé sans SMOTE\n",
    "print(f\"\\n COMPARAISON SMOTE vs MODÈLE OPTIMISÉ:\")\n",
    "print(f\"{'' * 80}\")\n",
    "print(f\"  Recall: {recall_1_opt:.4f} → {recall_smote:.4f} (gain: {(recall_smote - recall_1_opt)*100:+.2f}%)\")\n",
    "print(f\"  F1-Score: {f1_1_opt:.4f} → {f1_smote:.4f} (gain: {(f1_smote - f1_1_opt)*100:+.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aac5f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "=== TECHNIQUE COMBINÉE: AJUSTEMENT SEUIL + GB + SMOTE ===\n",
      "================================================================================\n",
      "\n",
      " TEST DE DIFFÉRENTS SEUILS SUR LE MODÈLE GB+SMOTE:\n",
      "\n",
      "Seuil      Precision       Recall          F1-Score        Accuracy       \n",
      "\n",
      "0.50       0.3671          0.2283          0.2815          0.7647         \n",
      "0.40       0.3328          0.4988          0.3992          0.6969         \n",
      "0.30       0.3037          0.7578          0.4336          0.6004         \n",
      "0.25       0.2955          0.8400          0.4372          0.5633         \n",
      "0.20       0.2853          0.8976          0.4330          0.5254         \n",
      "0.15       0.2739          0.9352          0.4237          0.4864         \n",
      "\n",
      "\n",
      " MEILLEUR SEUIL POUR LE RECALL:\n",
      "  Seuil optimal: 0.15\n",
      "  Precision: 0.2739 (27.39%)\n",
      "  Recall: 0.9352 (93.52%)\n",
      "  F1-Score: 0.4237\n",
      "  Accuracy: 0.4864 (48.64%)\n",
      "\n",
      " MEILLEUR SEUIL POUR LE F1-SCORE:\n",
      "  Seuil optimal: 0.25\n",
      "  Precision: 0.2955 (29.55%)\n",
      "  Recall: 0.8400 (84.00%)\n",
      "  F1-Score: 0.4372\n",
      "  Accuracy: 0.5633 (56.33%)\n",
      "\n",
      " COMPARAISON GB+SMOTE vs GB+SMOTE+SEUIL:\n",
      "\n",
      "  Recall: 0.2283 → 0.9352 (gain: +70.69%)\n",
      "  F1-Score: 0.2815 → 0.4372 (gain: +15.56%)\n",
      "  Precision: 0.3671 → 0.2739 (variation: -9.32%)\n",
      "\n",
      " MATRICE DE CONFUSION AVEC SEUIL OPTIMAL (0.15):\n",
      "\n",
      "\n",
      "                      Prédit: Présent (0)    Prédit: Absent (1)\n",
      "  Réel: Présent (0)           6577                  11065     \n",
      "  Réel: Absent  (1)            289                   4174     \n",
      "\n",
      "  Interprétation:\n",
      "    Vrais Négatifs (VN): 6577 - Correctement prédits présents\n",
      "    Vrais Positifs (VP): 4174 - Correctement prédits absents\n",
      "    Faux Positifs (FP): 11065 - Prédits absents mais venus (coût: rappel inutile)\n",
      "    Faux Négatifs (FN): 289 - Prédits présents mais absents (coût: rdv perdu)\n",
      "\n",
      "  Taux de détection des absents: 4174/4463 = 93.52%\n",
      "  Taux de fausses alertes: 11065/17642 = 62.72%\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"=== TECHNIQUE COMBINÉE: AJUSTEMENT SEUIL + GB + SMOTE ===\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Obtenir les probabilités du modèle GB+SMOTE\n",
    "y_proba_smote = gb_smote.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "# Test de différents seuils\n",
    "thresholds_to_test = [0.5, 0.4, 0.3, 0.25, 0.2, 0.15]\n",
    "results_threshold_smote = []\n",
    "\n",
    "print(\"\\n TEST DE DIFFÉRENTS SEUILS SUR LE MODÈLE GB+SMOTE:\")\n",
    "print(f\"{'' * 80}\")\n",
    "print(f\"{'Seuil':<10} {'Precision':<15} {'Recall':<15} {'F1-Score':<15} {'Accuracy':<15}\")\n",
    "print(f\"{'' * 80}\")\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    y_pred_threshold = (y_proba_smote >= threshold).astype(int)\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred_threshold, pos_label=1)\n",
    "    recall = recall_score(y_test, y_pred_threshold, pos_label=1)\n",
    "    f1 = f1_score(y_test, y_pred_threshold, pos_label=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "    \n",
    "    results_threshold_smote.append({\n",
    "        'threshold': threshold,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "    \n",
    "    print(f\"{threshold:<10.2f} {precision:<15.4f} {recall:<15.4f} {f1:<15.4f} {accuracy:<15.4f}\")\n",
    "\n",
    "print(f\"{'' * 80}\")\n",
    "\n",
    "# Trouver le meilleur seuil pour le recall\n",
    "best_recall_threshold = max(results_threshold_smote, key=lambda x: x['recall'])\n",
    "print(f\"\\n MEILLEUR SEUIL POUR LE RECALL:\")\n",
    "print(f\"  Seuil optimal: {best_recall_threshold['threshold']}\")\n",
    "print(f\"  Precision: {best_recall_threshold['precision']:.4f} ({best_recall_threshold['precision']*100:.2f}%)\")\n",
    "print(f\"  Recall: {best_recall_threshold['recall']:.4f} ({best_recall_threshold['recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score: {best_recall_threshold['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {best_recall_threshold['accuracy']:.4f} ({best_recall_threshold['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# Trouver le meilleur seuil pour le F1-Score\n",
    "best_f1_threshold = max(results_threshold_smote, key=lambda x: x['f1'])\n",
    "print(f\"\\n MEILLEUR SEUIL POUR LE F1-SCORE:\")\n",
    "print(f\"  Seuil optimal: {best_f1_threshold['threshold']}\")\n",
    "print(f\"  Precision: {best_f1_threshold['precision']:.4f} ({best_f1_threshold['precision']*100:.2f}%)\")\n",
    "print(f\"  Recall: {best_f1_threshold['recall']:.4f} ({best_f1_threshold['recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score: {best_f1_threshold['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {best_f1_threshold['accuracy']:.4f} ({best_f1_threshold['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# Comparaison avec GB+SMOTE seul (seuil par défaut 0.5)\n",
    "print(f\"\\n COMPARAISON GB+SMOTE vs GB+SMOTE+SEUIL:\")\n",
    "print(f\"{'' * 80}\")\n",
    "print(f\"  Recall: {recall_smote:.4f} → {best_recall_threshold['recall']:.4f} (gain: {(best_recall_threshold['recall'] - recall_smote)*100:+.2f}%)\")\n",
    "print(f\"  F1-Score: {f1_smote:.4f} → {best_f1_threshold['f1']:.4f} (gain: {(best_f1_threshold['f1'] - f1_smote)*100:+.2f}%)\")\n",
    "print(f\"  Precision: {precision_smote:.4f} → {best_recall_threshold['precision']:.4f} (variation: {(best_recall_threshold['precision'] - precision_smote)*100:+.2f}%)\")\n",
    "\n",
    "# Matrice de confusion pour le meilleur seuil (recall optimisé)\n",
    "print(f\"\\n MATRICE DE CONFUSION AVEC SEUIL OPTIMAL ({best_recall_threshold['threshold']}):\")\n",
    "print(f\"{'' * 80}\")\n",
    "\n",
    "# Générer les prédictions avec le seuil optimal\n",
    "y_pred_optimal = (y_proba_smote >= best_recall_threshold['threshold']).astype(int)\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
    "vn_opt, fp_opt, fn_opt, vp_opt = cm_optimal.ravel()\n",
    "\n",
    "print(f\"\\n                      Prédit: Présent (0)    Prédit: Absent (1)\")\n",
    "print(f\"  Réel: Présent (0)         {vn_opt:>6}                 {fp_opt:>6}     \")\n",
    "print(f\"  Réel: Absent  (1)         {fn_opt:>6}                 {vp_opt:>6}     \")\n",
    "\n",
    "print(f\"\\n  Interprétation:\")\n",
    "print(f\"    Vrais Négatifs (VN): {vn_opt} - Correctement prédits présents\")\n",
    "print(f\"    Vrais Positifs (VP): {vp_opt} - Correctement prédits absents\")\n",
    "print(f\"    Faux Positifs (FP): {fp_opt} - Prédits absents mais venus (coût: rappel inutile)\")\n",
    "print(f\"    Faux Négatifs (FN): {fn_opt} - Prédits présents mais absents (coût: rdv perdu)\")\n",
    "\n",
    "print(f\"\\n  Taux de détection des absents: {vp_opt}/{vp_opt + fn_opt} = {best_recall_threshold['recall']*100:.2f}%\")\n",
    "print(f\"  Taux de fausses alertes: {fp_opt}/{vn_opt + fp_opt} = {fp_opt/(vn_opt + fp_opt)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4990302",
   "metadata": {},
   "source": [
    "##  CONCLUSION\n",
    "\n",
    "Ce projet a permis d'optimiser un modèle **Gradient Boosting** pour prédire les absences aux rendez-vous médicaux. \n",
    "\n",
    "**Résultats clés:**\n",
    "- GridSearchCV a optimisé les hyperparamètres (learning_rate, max_depth, n_estimators)\n",
    "- SMOTE a rééquilibré le dataset (80/20 → 67/33)\n",
    "- L'ajustement du seuil de décision a maximisé le Recall\n",
    "- La combinaison des 3 techniques améliore significativement la détection des absents\n",
    "\n",
    "**Recommandation:** Utiliser le modèle GB+SMOTE avec un seuil ajusté entre 0.15 et 0.25 selon le compromis souhaité entre Recall (détection) et Precision (fausses alertes)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
